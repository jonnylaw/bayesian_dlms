<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Student’s t-distributed DLM</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>




<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Bayesian Inference for DLMs</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="model_building_simulation.html">Model Building</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Filtering
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="kalman_filter.html">Kalman Filter</a>
    </li>
    <li>
      <a href="particle_filter.html">Bootstrap Particle Filter</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Parameter Learning
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="gibbs_sampling.html">Gibbs Sampling</a>
    </li>
    <li>
      <a href="pmmh.html">PMMH</a>
    </li>
  </ul>
</li>
<li>
  <a href="forecasting.html">Forecasting</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Examples
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="FirstOrderDlm.html">First Order DLM</a>
    </li>
    <li>
      <a href="second_order_dlm.html">Linear Growth DLM</a>
    </li>
    <li>
      <a href="seasonal_dlm.html">Seasonal Composed DLM</a>
    </li>
    <li>
      <a href="CorrelatedModel.html">Multivariate DLM</a>
    </li>
    <li>
      <a href="student_t_filtering.html">Student's t-distributed DLM</a>
    </li>
    <li>
      <a href="ar1.html">Autoregressive Model</a>
    </li>
    <li>
      <a href="spain_investment_data.html">Spain Investment Data</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Student’s t-distributed DLM</h1>

</div>


<p>The Student’s t-distribution is not part of the exponential family of distributions, hence is not a DGLM (Dynamic generalised linear model), however it is still useful in its own right. The Student’s t-distribution has thicker tails than the normal distribution for low values of the degrees of freedom hence is more foregiving of outliers in the measurements. The scaled, shifted Student’s t-distribution is parameterised by the location, <span class="math inline">\(\ell\)</span>, scale <span class="math inline">\(s\)</span> and degrees of freedom <span class="math inline">\(\nu\)</span>, the probability density function is:</p>
<p><span class="math display">\[p(x|\ell, s, \nu) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\pi\nu s}\Gamma(\frac{\nu}{2})} \left(\frac{(x-\ell)^2}{\nu s^2} + 1 \right)^{- \frac{\nu + 1}{2}}\]</span></p>
<p>Then the state-space model, with a first order polynomial latent-state evolution can be written as:</p>
<p><span class="math display">\[\begin{aligned}
Y_t &amp;\sim t_\nu(x_t, s^2), \\
x_t &amp;= x_{t-1} + w_t, \qquad w_t \sim \mathcal{N}(0, W), \\
x_0 &amp;\sim \mathcal{N}(m_0, C_0).
\end{aligned}\]</span></p>
<p>The Student’s t model is specified in scala by first specifying a DLM model, which specifys the system and observation matrices (<span class="math inline">\(G_t\)</span> and <span class="math inline">\(F_t\)</span>) then specifying the observation model using the <code>Dglm</code> class:</p>
<pre class="tut:silent"><code>import dlm.model._

val dlm = Dlm.polynomial(1)
val mod = Dglm.studentT(3, dlm)</code></pre>
<p>The code below can be used to simulate 1,000 iterations from the Student’s t model.</p>
<pre class="tut:silent"><code>import breeze.linalg.{DenseVector, DenseMatrix}

val params = Dlm.Parameters(
  DenseMatrix(3.0), 
  DenseMatrix(0.1), 
  DenseVector(0.0), 
  DenseMatrix(1.0))


val data = Dglm.simulate(mod, params).
  steps.
  take(1000).
  toVector</code></pre>
<p>A simulation from the model with <span class="math inline">\(\{s^2, W, m_0, C_0\} = \{3, 0.1, 0, 1\}\)</span> is presented in the figure below:</p>
<p><img src="student_t_filtering_files/figure-html/plot_student_t-1.png" width="672" /></p>
<div id="parameter-learning" class="section level2">
<h2>Parameter learning</h2>
<p>In order to fit the t-distributed state space model to observed data we need to learn the values of the parameters <span class="math inline">\(\psi = \{s^2, W, m_0, C_0\}\)</span>. A particle filter can be used to find an (unbiased) estimate of the likelihood, then Metropolis Hastings can be used to learn the posterior distribution of the parameters. Alternatively, Particle Gibbs can be used to sample a value of the state then the conjugate structure of the latent-state with unknown variance, <span class="math inline">\(W\)</span> can be exploited. The scale would then be updated using Metropolis hastings. However, since the Student’s t-distrubtion is an Inverse Gamma mixture of normals, we can use the Kalman Filter to sample the value of the state and perform Gibbs sampling for the value of the scale, <span class="math inline">\(s\)</span>.</p>
<div id="kalman-filtering" class="section level3">
<h3>Kalman Filtering</h3>
<p>Student’s t-distribution arises as a Inverse Gamma mixture of Normals, consider <span class="math inline">\(X | V \sim \mathcal{N}(\mu, V)\)</span>, <span class="math inline">\(W \sim \textrm{Inverse-Gamma}(\alpha, \beta)\)</span> then the marginal distribution of <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
p(X) &amp;= \int_0^\infty p(X|V)p(V) \textrm{d}V, \\
&amp;= \int_0^\infty \frac{\beta^\alpha}{\sqrt{2\pi}\Gamma(\alpha)}  V^{-\frac{1}{2}} \exp \left(\frac{-(x-\mu)^2}{2V}\right) V^{-\alpha-1} \exp \left( -\frac{\beta}{V} \right) \textrm{d}V, \\
&amp;= \frac{\beta^\alpha}{\sqrt{2\pi}\Gamma(\alpha)}\int_0^\infty V^{-(\alpha + \frac{1}{2}) - 1} \exp \left\{-\frac{1}{V}\left(\frac{(x-\mu)^2}{2} + \beta \right)\right\} \textrm{d}V,\\
&amp;= \frac{\beta^\alpha\Gamma\left(\alpha + \frac{1}{2}\right)}{\sqrt{2\pi}\Gamma(\alpha)\left(\frac{(x-\mu)^2}{2} + \beta \right)^{\alpha + \frac{1}{2}}
} \\
&amp;= \frac{\beta^\alpha\Gamma\left(\alpha + \frac{1}{2}\right)}{\sqrt{2\pi}\Gamma(\alpha)\beta^{\alpha + \frac{1}{2}}\left(\frac{(x-\mu)^2}{2\beta} + 1 \right)^{\alpha + \frac{1}{2}}} \\
&amp;= \frac{\Gamma\left(\frac{2\alpha + 1}{2}\right)}{\sqrt{2\pi\beta}\Gamma(\alpha)} \left(\frac{(x-\mu)^2}{2\beta} + 1 \right)^{- \frac{2\alpha + 1}{2}}
\end{aligned}\]</span></p>
<p>Then comparing this result with the PDF of the scaled, shifted Student’s t-distribution we can see that, <span class="math inline">\(\nu = 2\alpha\)</span>, <span class="math inline">\(\ell = \mu\)</span> and <span class="math inline">\(s = \sqrt{\beta/\alpha}\)</span>.</p>
<p>This derivation means we have found a way to simulate from a scaled, shifted Student’s t-distribution, by first simulating <span class="math inline">\(V_i\)</span> from the Inverse Gamma distribution with parameters <span class="math inline">\(\alpha = \nu/2\)</span> and <span class="math inline">\(\beta = \nu s^2/2\)</span>, then simulating from a Normal distribution with mean <span class="math inline">\(\ell\)</span> and variance <span class="math inline">\(V_i\)</span>. Further, this means we can use Kalman Filtering for a model with a Student-t observation distribution.</p>
</div>
<div id="exact-filtering-for-the-students-t-distribution-model" class="section level3">
<h3>Exact Filtering for the Student’s t-distribution Model</h3>
<p>A Dynamic linear model with Student’s t-Distribution observation noise can be written as:</p>
<p><span class="math display">\[\begin{aligned}
Y_t | \textbf{x}_t &amp;\sim t_\nu(F_t \textbf{x}_t, s),\\
\textbf{X}_t | \textbf{x}_{t-1} &amp;= G_t\textbf{x}_{t-1} + \textbf{w}_t, &amp;\textbf{w}_t &amp;\sim \textrm{MVN}(0, W t),\\
\textbf{X}_0 &amp;\sim \textrm{MVN}(m_0, C_0).
\end{aligned}\]</span></p>
<p>Where the location of the Student’s t-distribution is <span class="math inline">\(F_t\textbf{x}_t\)</span>, the scale and degrees of freedom are static and can be learned from the data. Consider the measurement update step of the Kalman filter with the new observation distribution:</p>
<p><span class="math display">\[p(Y_t|\textbf{x}_t) = \int_0^\infty \mathcal{N}\left(Y_t; F_t\textbf{x}_t, V_t\right)\textrm{InverseGamma}\left(V_t;\frac{\nu}{2}, \frac{\nu s^2}{2}\right) \textrm{d}V_t\]</span></p>
<p>Using the result in the previous section, that the Student’s t-distribution is an Inverse Gamma mixture of Normals. The joint distribution of all the random variables in the model can be written as:</p>
<p><span class="math display">\[\begin{aligned}
p(\textbf{x}_{0:T}, y_{1:T}, s, W) &amp;= p(W)p(x_0)\prod_{t=1}^Tp(V_t)p(Y_t|\textbf{x}_t, V_t)p(\textbf{x}_t|\textbf{x}_{t-1}, W) \\
&amp;= \textrm{InverseGamma}(W_j; \alpha, \beta) \mathcal{N}(\textbf{x}_0; m_0, C_0)\times\\ &amp;\prod_{t=1}^T\textrm{InverseGamma}\left(V_t; \frac{\nu}{2}, \frac{\nu s^2}{2}\right)\mathcal{N}(Y_t; F_t\textbf{x}_t, V_t)\mathcal{N}(\textbf{x}_t; G_t\textbf{x}_{t-1}, W)
\end{aligned}\]</span></p>
<p>Using the Markovian property of the latent-state and the conditional independence in the model. Construct a Gibbs Sampler, the state can be sampled conditional on the observed data <span class="math inline">\(y_{1:T}\)</span> and the parameters <span class="math inline">\(W\)</span> and <span class="math inline">\(V_t\)</span> using FFBS.</p>
<p>For the system noise matrix, assume the matrix is diagonal, with each diagonal element indexed by <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
p(W_j|\textbf{x}_{0:T}, y_{1:T}, s, V) &amp;= p(W)\prod_{t=1}^Tp(\textbf{x}_t|\textbf{x}_{t-1}, W) \\
&amp;= \textrm{InverseGamma}(\alpha, \beta)\prod_{t=1}^T\mathcal{N}(G_t\textbf{x}_{t-1}, W) \\
&amp;= \textrm{InverseGamma}\left(\alpha + \frac{T}{2}, \beta + \frac{1}{2}\sum_{i=1}^t (\textbf{x}_t - G_t\textbf{x}_{t-1})_j^2 \right).
\end{aligned}\]</span></p>
<p>For the auxiliary variable, <span class="math inline">\(V_t\)</span>, the variance of the Normal distribution:</p>
<p><span class="math display">\[\begin{aligned}
p(V|\textbf{x}_{0:T}, y_{1:T}, W, s) &amp;= \prod_{t=1}^Tp(Y_t|\textbf{x}_t, V_t)p(V_t) \\
&amp;= \prod_{t=1}^T\textrm{InverseGamma}\left(\frac{\nu}{2}, \frac{\nu s^2}{2}\right)\mathcal{N}(F_t\textbf{x}_t, V_t) 
\end{aligned}\]</span></p>
<p>Then for each observation of the process, we have to sample a value of the variance from:</p>
<p><span class="math display">\[\begin{aligned}
p(V_t|\textbf{x}_{0:T}, y_{1:T}, W, s) &amp;= p(Y_t|\textbf{x}_t, V_t)p(V_t), \\
&amp;= \textrm{InverseGamma}\left(\frac{\nu + 1}{2}, \frac{\nu s^2 + (y_t - F_t\textbf{x}_t)^2}{2}\right).
\end{aligned}\]</span></p>
<p>For the scale of the Student’s t-distribution, <span class="math inline">\(s^2\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
p(s^2|y_{1:T}, \textbf{x}_{0:T}, W, V) &amp;= \prod_{t=1}^T\textrm{InverseGamma}\left(V_t; \frac{\nu}{2}, \frac{\nu s^2}{2}\right) \\
&amp;\propto \prod_{t=1}^T \left(\frac{\nu s^2}{2}\right)^{\frac{\nu}{2}} \exp\left(-\frac{\nu s^2}{2V_t}\right) \\
&amp;= \left(\frac{\nu s^2}{2}\right)^{\frac{T\nu}{2}}\exp\left(-\frac{\nu}{2} \sum_{i=1}^T\frac{s^2}{V_t}\right) \\
&amp;\propto (s^2)^{T\nu/2}\exp\left(-\frac{\nu}{2} \sum_{i=1}^T\frac{s^2}{V_t}\right) \\
&amp;\propto \textrm{Gamma}\left(\frac{T\nu}{2} + 1, \frac{\nu}{2} \sum_{i=1}^T\frac{1}{V_t}\right)
\end{aligned}\]</span></p>
<p>Note that we have assumed the degrees of freedom of the Student’s t-distribution (<span class="math inline">\(\nu\)</span>) is known a-priori. This can also be learned from the data using a Metropolis-Hastings step.</p>
<p>The Kalman Filter needs to incorporate the change of variance at each timestep. The step which has to be changed is the one-step prediction, once we have sampled the variances, <span class="math inline">\(V_t\)</span> for each observation from the posterior distribution derived above, we can perform the Kalman filter conditional upon knowing these variances:</p>
<p><span class="math display">\[\begin{aligned}
\textrm{Prediction:} &amp; &amp; \textrm{draw } V_t &amp;\sim \textrm{InverseGamma}\left(\frac{\nu + 1}{2}, \frac{\nu s^2 + (y_t - F_t\textbf{x}_t)^2}{2}\right). \\
&amp; &amp; f_t &amp;= F(t) a_t \\
                     &amp; &amp; Q_t &amp;= F_t R_t F_t^\prime + V_t
\end{aligned}\]</span></p>
<p>This is sufficient to learn the parameters of a DGLM with a Student’s t-Distributed observation noise. The advantage to this approach is that we don’t have the computational complexity of using a Particle Filter for each iteration of the MCMC.</p>
<p>The prior distribution of the system evolution matrix is Inv-Gamma<span class="math inline">\((21, 2)\)</span>, for a mean of 0.1 and variance 0.19. The prior distribution for <span class="math inline">\(W\)</span> looks like:</p>
<p><img src="student_t_filtering_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>In order to run the perform Gibbs sampling on the sampled data, we use the following code:</p>
<pre class="tut:silent"><code>val iters = StudentT.sample(3, data.map(_._1), InverseGamma(21.0, 2.0), mod, params)</code></pre>
<p><img src="student_t_filtering_files/figure-html/density_student_t-1.png" width="672" /></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
