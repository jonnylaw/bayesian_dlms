---
title: "Correlated DLM"
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(cache=TRUE)
theme_set(theme_minimal())
```

# Problem Description

In order to model many related time series and perform forecasting jointly we can formulate a Dynamic Linear Model with an observation *vector* containing values from the related time series at each timestep. To induce correlation in the model, the State evolution matrix, $W$, is allowed to be full-rank. This can be learned from the data.

$$\begin{align}
\textbf{Y}_t &= F_t x_t + \textbf{v}_t, \quad v_t \sim \textrm{MVN}(0, V), \\
X_t &= G_t x_{t-1} + w_t, \quad w_t \sim \textrm{MVN}(0, W).
\end{align}$$

The observation at each time point consists of $n$ related time series, $\textbf{Y}_t = (y_{1,t},\dots,y_{n,t})$.

## Example

To illustrate the combination of single models into a combined model, we consider the "outer sum" of two models. The first model is a first order polynomial model, the second model in the composition is a linear trend. The first model is given by:

$$\begin{align}
Y_t &= x_t + v_t, \quad v_t \sim \textrm{MVN}(0, V), \\
X_t &= x_{t-1} + w_t, \quad w_t \sim \textrm{MVN}(0, W), \\
X_0 &\sim \textrm{MVN}(m_0, C_0).
\end{align}$$

Where the state, $x_t$ is univarate an evolves according to a random walk. The second model is given by: 

$$\begin{align}
Y_t &= F \textbf{x}_t + \textbf{v}_t, \quad v_t \sim \textrm{MVN}(0, V), \\
\textbf{X}_t &= G \textbf{x}_{t-1} + w_t, \quad w_t \sim \textrm{MVN}(0, W), \\
\textbf{X}_0 &\sim \textrm{MVN}(m_0, C_0).
\end{align}$$

The state is two dimensional, as such the system noise matrix $W$ is a $2 \times 2$ matrix. The observation matrix does not depend on time and is given by, $F = (1 \quad 0)$ and the system evolution matrix is:

$$G = \begin{pmatrix} 
1 & 1 \\
0 & 1
\end{pmatrix}.$$

The two DLMs above can be composed using an outer sum to model two time series which are thought to be related. The observation matrices are block-concatenated so that the composed model has as the observation matrix: 

$$F = \begin{pmatrix} 
1 & 0 \\
0 & 1 \\
0 & 0
\end{pmatrix}$$ 

The system evolution matrices of each model in the composition are block-concatenated so that the system matrix of the composed model is:

$$G = \begin{pmatrix} 
1 & 0 & 0 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{pmatrix}.$$

In order to compose these two models in the Scala package, we first define the two models:

```scala
val mod1 = Dlm.polynomial(1)
val mod2 = Dlm.polynomial(2)
```

The models can now be composed using the outer sum function:

```scala
val composedModel = Dlm.outerSum(mod1, mod2)

val p = Parameters(
  v = diag(DenseVector(1.0, 2.0)), 
  w = diag(DenseVector(2.0, 3.0, 1.0)),
  m0 = DenseVector.zeros[Double](2), 
  c0 = DenseMatrix.eye[Double](2)
)
```

Then we can simulate observations from the `composedModel`:

```scala
Dlm.simulate(0, composedModel, p).
  steps.
  take(1000)
```

The figure below shows a simulation from this composed model:

```{r, message=FALSE, warning=FALSE, echo=FALSE}
first_order_and_linear_trend = read_csv("../data/first_order_and_linear_trend.csv")

first_order_and_linear_trend %>%
  gather(key, value, observation_1, observation_2) %>%
  ggplot(aes(x = time, y = value)) +
  geom_line() +
  facet_wrap(~key, scales = "free_y", ncol = 1)
```

# Example 2

For simplicity we consider the outer sum of two first order DLMs. given by the following equation:

$$\begin{align*}
\begin{pmatrix}y_{1,t} \\ y_{2,t}\end{pmatrix} &= \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}\begin{pmatrix}x_{1,t} \\ x_{2,t}\end{pmatrix} + \begin{pmatrix}v_{1,t} \\ v_{2,t} \end{pmatrix}, \qquad \textbf{v} \sim \textrm{MVN}\left(0, \begin{pmatrix}V_1 & 0 \\ 0 & V_2\end{pmatrix}\right), \\
\begin{pmatrix}x_{1,t} \\ x_{2,t} \end{pmatrix} &= \begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix} \begin{pmatrix}x_{1,t-1} \\ x_{2,t-1} \end{pmatrix} + \begin{pmatrix} w_1 \\ w_2 \end{pmatrix}, \qquad \textbf{v} \sim \textrm{MVN}\left(0, \begin{pmatrix}W_1 & W_2 \\ W_3 & W_4\end{pmatrix}\right), \\
\textbf{x}_0 &\sim \textrm{MVN}(m_0, C_0).
\end{align*}$$

A simulation from this model is presented in the figure below:

```{r, message=FALSE, warning=FALSE, echo=FALSE}
data = read_csv("../data/correlated_dlm.csv")

data %>%
  gather(key, value, observation_1, observation_2) %>%
  ggplot(aes(x = time, y = value)) +
  geom_line() +
  facet_wrap(~key, scales = "free_y", ncol = 1)
```

# Kalman Filtering

Given that we have simulated the model, we can perform the Kalman filter to determine the filtering distribution of the latent state using the true value of the parameters. The figure below shows the filtering distribution with 90% probability intervals for time $t = 200, \dots , 300$.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
filtered = read_csv("../data/correlated_dlm_filtered.csv")

filtered %>%
  inner_join(data, by = "time") %>%
  filter(time > 200) %>%
  filter(time < 300) %>%
  mutate(upper = qnorm(p = 0.95, mean = state_mean, sd = sqrt(state_variance))) %>%
  mutate(lower = qnorm(p = 0.05, mean = state_mean, sd = sqrt(state_variance))) %>%
  gather(key, value, state_mean, state_1) %>%
  ggplot(aes(x = time, y = value, colour = key)) +
  geom_line() +
  geom_line(aes(x = time, y = lower), linetype = 3, colour = "#000000") +
  geom_line(aes(x = time, y = upper), linetype = 3, colour = "#000000") +
  theme(legend.position = "bottom") +
  labs(title = "Kalman Filtering the Correlated Model", 
       subtitle = "Mean of the Filtering Distribution overlayed with the actual simulated state with 90% probability intervals")
```


## Parameter Learning

The unkown parameters of the model are the system noise matrix, $W$ and the observation noise matrix, $V$. The observation noise matrix is assumed to be diagonal, meaning the measurement noise of each process is considered to be independent. We can use Gibbs Sampling to determine the values of $W$ and $V$, by utilising conjugate structure of the conditional distributions of the model. Assume initially that the parameters of the initial state are known, then the unknown parameters can be written as the vector, $\theta = (v_1, v_2, w_1, \dots, w_9)$. The joint distribution of all the random variables in the model can be written as:

$$p(\textbf{X}_{1:T}, \textbf{Y}_{1:T}, \theta) = p(\theta) p(\textbf{x}_0) \prod_{i=1}^T p(\textbf{y}_i| \textbf{x}_i, \theta) p(\textbf{x}_i | \textbf{x}_{i-1}, \theta)$$

This factorisation of the random variables makes it clear to see how a gibbs sampling approach could be used. We observe values of the observations $Y_{1:T}$ from time $t = \{1, \dots, T\}$, then given the value of the parameters we sample a value of the state using forward filtering - backward sampling (FFBS). Then given the values of the states, we can draw a value for both the observation and system noise covariances. 

## Observation Noise Matrix: d-Inverse Gamma

First consider the observation variance matrix, this matrix is diagonal and hence we only need to learn the variances. The following steps are simplified by considering only one time series in the observation vector. The prior distribution of the observation variance, $V$, is the Inverse Gamma distribution:

$$p(V) = \textrm{InverseGamma}(\alpha, \beta)$$

The likelihood of $y_t$ is Gaussian, with mean $F^T \textbf{x}_t$ and variance $V$. The Inverse Gamma distribution is conjugate to the Normal distribution with known mean and unknown variance. The posterior distribution of the observation variance is:

$$\begin{align*}
p(V | y_{1:T}, \textbf{x}_{0:T}) &\propto  p(x_0) p(V) \prod_{t=1}^Tp(y_t | V, x_t) p(x_t|x_{t-1}) \\
&= V^{-\alpha-1}\exp\left( -\frac{\beta}{V} \right)(2\pi V)^{-T/2} \exp \left\{ -V^{-1} \sum_{t=1}^T(y_t - F_t \textbf{x}_t)^2 \right\} \\
&= V^{-(\alpha + T/2) - 1} \exp \left\{ -\frac{1}{V}\left(\beta + \frac{1}{2}\sum_{t=1}^T(y_t - F_t \textbf{x}_t)^2\right) \right\}
\end{align*}$$

The the posterior distribution of the observation variance is the Inverse Gamma distribution:

$$p(V | y_{1:T}, \textbf{x}_{1:T}) = \textrm{InverseGamma}\left(\alpha + \frac{T}{2}, \beta + \frac{1}{2}\sum_{i=1}^T(y_t - F_t\textbf{x}_t)^2\right).$$

## System Noise Matrix: Inverse Wishart Prior

For the system noise matrix, the Inverse Wishart distribution can be used as the conjugate prior for the Multivariate Normal likelihood for $\textbf{x}_t$ with unknown covariance, $W$ and known mean $G_t\textbf{x}_{t-1}$. The prior on W is written as:

$$p(W) \sim \mathcal{W}^{-1}(\mathbf{psi}, \nu)$$

The PDF of the inverse wishart distribution is given as follows:

$$p(W) = \frac{\left|{\mathbf\Psi}\right|^{\frac{\nu}{2}}}{2^{\frac{\nu p}{2}}\Gamma_p(\frac{\nu}{2})} \left|W\right|^{-\frac{\nu+p+1}{2}}\textrm{exp}\left\{-\frac{1}{2}\operatorname{tr}({\mathbf\Psi}W^{-1})\right\}.$$
Where $p$ is the dimension of the matrix $W$, in this $p = 2$. The posterior distribution for the parameter $W$ given the values of the state, $x_{1:T}$ can be written as:

$$\begin{align*}p(W|x_{1:T}) &\propto p(x_0)p(W)\prod_{i=1}^T p(x_i|x_{i-1}, W) \\
&= \left|W\right|^{-\frac{\nu+p+1}{2}}\textrm{exp}\left\{-\frac{1}{2}\operatorname{tr}({\mathbf\Psi}W^{-1})\right\} \\ &\times\left|{W}\right|^{-\frac{T}{2}}\textrm{exp}\left\{ -\frac{1}{2}\sum_{i=1}^T(x_i - Gx_{i-1})^TW^{-1}(x_i-Gx_{i-1})\right\} \\
&= \left|W\right|^{\frac{\nu + T + 2 + 1}{2}}\textrm{exp}\left\{ -\frac{1}{2}\operatorname{tr}(\mathbf{\Psi}W^{-1}) - \operatorname{tr}\left(\frac{1}{2}\sum_{i=1}^T(x_i - Gx_{i-1})(x_i-Gx_{i-1})^TW^{-1}\right)\right\}
\end{align*}$$

The final line uses standard results from Multivariate statistics and the following rule for matrix traces: 

* A Matrix trace is invariant under cyclic permutations, $\operatorname{tr}(ABC) = \operatorname{tr}(CAB) = \operatorname{tr}(BCA)$

This can be futher simplified to:

$$p(W|x_{1:T}) \propto \left|W\right|^{\frac{\nu + T + 2 + 1}{2}}\textrm{exp}\left\{ -\frac{1}{2}\operatorname{tr}((\mathbf{\Psi} + \sum_{i=1}^T(x_i - Gx_{i-1})(x_i-Gx_{i-1})^T) W^{-1}) \right\}.$$

This final simplification uses another couple of facts about the matrix trace:

* The sum of the trace is equal to the trace of the sum, $\operatorname{tr}(A + B) = \operatorname{tr}(A) + \operatorname{tr}(B)$
* $\operatorname{tr}(cA) = c\operatorname{tr}(A)$ for a constant "c"

We recognise the posterior distribution for the system noise covariance as the Inverse Wishart distribution:

$$p(W|x_{1:T}) = \mathcal{W}^{-1}(\nu + T, \mathbf{\Psi} + \sum_{i=1}^T(x_i - Gx_{i-1})(x_i-Gx_{i-1})^T)$$.

Now in order to perform gibbs sampling, we alternate between drawing values of $V, W$ from these posterior distributions and values of the state $x_0,\dots,x_T$ using forward filtering backward sampling. 

## Example: Parameter Learning in a simulated Model

```{r, fig.cap="Diagnostic plots for the MCMC draws from the posterior distribution of the non-zero diagonal elements of the Observation noise covariance matrix for the simulated correlated model", message=FALSE, warning=FALSE, echo=FALSE}
iters = read_csv("../data/correlated_dlm_gibbs.csv")

actual_values = tibble(
  parameter = c("V1", "V4", paste0("W", 1:4)),
  actual_value = c(1.0, 4.0, 0.75, 0.5, 0.5, 1.25)
)

params = iters %>%
  mutate(iteration = 1:nrow(iters)) %>%
  gather(key = parameter, value, -iteration) %>%
  inner_join(actual_values, by = "parameter") %>%
  filter(parameter %in% c("V1", "V4"))

p1 = params %>%
  ggplot(aes(x = iteration, y = value)) +
  geom_line() +
  geom_hline(aes(yintercept = actual_value), colour = "#ff0000") +
  facet_wrap(~parameter, scales = "free_y")

p2 = params %>%
  group_by(parameter) %>%
  mutate(running_mean = dlm::ergMean(value)) %>%
  ggplot(aes(x = iteration, y = running_mean)) +
  geom_line() +
  geom_hline(aes(yintercept = actual_value), colour = "#ff0000") +
  facet_wrap(~parameter, nrow = 1, scales = "free_y")

gridExtra::grid.arrange(p1, p2, ncol = 1)
```

```{r, fig.cap="Diagnostic plots for the MCMC draws from the posterior distribution of the System noise covariance matrix for the simulated correlated model", message=FALSE, warning=FALSE, echo=FALSE}
params = iters %>%
  mutate(iteration = 1:nrow(iters)) %>%
  gather(key = parameter, value, -iteration) %>%
  inner_join(actual_values, by = "parameter") %>%
  filter(parameter %in% paste0("W", 1:4))

p1 = params %>%
  ggplot(aes(x = iteration, y = value)) +
  geom_line() +
  geom_hline(aes(yintercept = actual_value), colour = "#ff0000") +
  facet_wrap(~parameter, scales = "free_y")

p2 = params %>%
  group_by(parameter) %>%
  mutate(running_mean = dlm::ergMean(value)) %>%
  ggplot(aes(x = iteration, y = running_mean)) +
  geom_line() +
  geom_hline(aes(yintercept = actual_value), colour = "#ff0000") +
  facet_wrap(~parameter, nrow = 1, scales = "free_y")

gridExtra::grid.arrange(p1, p2, ncol = 1)
```